# 香农熵

公式：

$$
H(X) = -\sum_{i= 1}^{n}p(x_i) \cdot \log_{_2}{p(x_i)}
$$

如果$H(X)$ 越大，则其混乱程度越大，不确定性越大

#### ID3

> 越是小型的决策树越优于大的决策树`奥卡姆剃刀思想`
> 
> ID3 算法可以归纳为以下几点：
> 
> 1. 使用所有没有使用的属性并计算与之相关的样本熵值
> 2. 选取其中熵值最小的属性
> 3. 生成包含该属性的节点

公式：

$$
H(X) = -\sum_{i= 1}^{n}(x_i) \cdot \log_{_2}{p(x_i)}
$$

特征 A 对 D 的信息增益：

$$
Gain(D, A) = H(D) - \sum_{v = 1}^{n}\frac{| D_v | }{|D|}\cdot H(D_v)
$$

> 在 ID3 算法中，我们会计算每个特征的信息增益，并选择具有最大信息增益的特征作为当前节点的划分特征。这样可以使得在该节点的划分后，数据集的不确定性减少最多，从而使得决策树的纯度提高。

#### C4.5

> 检查上述基本情况
> 对于每个特征 a，计算划分 a 的信息增益
> 记 a_best 为最高信息增益的特征
> 创建一个在 a_best 上划分的决策节点
> 使用划分后的样本创建作为当前决策节点的子节点，并在这些子节点上递归地处理

#### CCP 剪枝

成本复杂度：

$$
R_α(t)=R(t)+α∣T_t| \tag{2}
​
$$

其中 ：

$R(t)$是基尼系数

$|T_t|$则表示 t 样本数量，

$α$表示超参数，平衡复杂度和性能

> 从叶子节点开始从下到上依次遍历计算公式 `(2)` ,选择剪枝后成本复杂度最小的开始剪枝，如果多节点复杂度相同，则选其中一个。
> 
> 选择节点删除其子树，该节点成为叶子节点，其中标签种类最多的为其标签。
